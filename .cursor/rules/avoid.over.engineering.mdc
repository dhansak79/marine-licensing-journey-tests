---
description: This rule applies when creating test data models, factories, or any test infrastructure code. It's easy to get excited and create comprehensive, feature-rich test utilities that end up being unused.
globs: 
alwaysApply: false
---
# Avoid Over-Engineering in Test Code

*Based on refactoring experience with ExemptionModel*

## Context
This rule applies when creating test data models, factories, or any test infrastructure code. It's easy to get excited and create comprehensive, feature-rich test utilities that end up being unused.

## Core Principles

### 1. Start Simple, Grow as Needed
- Begin with the minimal viable implementation
- Only add complexity when there's a clear, immediate need
- Resist the urge to build "just in case" features

### 2. Follow YAGNI (You Aren't Gonna Need It)
- Don't create comprehensive test scenarios until they're actually used
- Don't build persona-based data generators unless personas from `documentation/personas/` are actively used in tests
- Don't create boundary testing utilities until boundary tests exist

### 3. Align with Actual Usage Patterns
- Check how existing tests actually work before building new infrastructure
- If tests use simple factories, don't build complex model generators
- If tests use hardcoded values, question whether dynamic generation is needed

## Warning Signs of Over-Engineering

### In Test Data Models
- **Multiple unused generation methods** (generateTestScenarios, generateForPersona, generateBoundaryTestData)
- **Complex inheritance hierarchies** for simple data structures
- **Comprehensive validation logic** that duplicates application validation
- **Extensive configuration options** that are never used

### In Test Factories
- **Dozens of static factory methods** when only 2-3 are used
- **Deep object composition** when simple objects suffice
- **Realistic data generation** when fake data works fine
- **Cross-model dependencies** that create maintenance overhead

### In Test Infrastructure
- **Abstract base classes** for test utilities
- **Plugin architectures** for simple test helpers
- **Configuration files** for hardcoded test values
- **Comprehensive error handling** in test utilities

### Code Quality Issues
- **Duplicate methods** with different names doing the same thing
- **Inconsistent APIs** mixing parameter types (boolean vs string)
- **Mixed error handling** using both `throw Error` and `expect.fail()`
- **Inconsistent naming** for similar functionality across classes

## Better Approaches

### Start with the Minimum
```javascript
// Instead of this over-engineered approach:
class ExemptionModel {
  static generateCompleteExemption(options = {}) { /* 50 lines */ }
  static generateTestScenarios() { /* 100 lines */ }
  static generateForPersona(persona) { /* 80 lines */ }
  static generateBoundaryTestData() { /* 60 lines */ }
  static generateInvalidData() { /* 40 lines */ }
}

// Start with this:
class ExemptionModel {
  constructor(data = {}) {
    this.id = data.id || faker.database.mongodbObjectId()
    this.projectName = data.projectName || null
    // ... only what's actually needed
  }
}
```

### Use Simple Factories
```javascript
// Instead of comprehensive generation:
static generateCompleteExemption(options = {}) {
  // Complex logic with many options
}

// Use focused factories:
static withValidProjectName() {
  return new ApplyForExemption({ projectName: 'Test Project' })
}

static withEmptyProjectName() {
  return new ApplyForExemption({ projectName: '' })
}
```

### Build Incrementally
1. **Write the test first** - see what data you actually need
2. **Create minimal data** - hardcode values initially
3. **Extract patterns** - only when you see repetition
4. **Add generation** - only when variation is needed

## Refactoring Guidelines

### When to Simplify
- **Unused methods** - delete them immediately
- **Complex options** - replace with simple, focused methods
- **Comprehensive scenarios** - keep only what tests actually use
- **Cross-dependencies** - break them down to simpler, independent pieces

### How to Refactor
1. **Audit actual usage** - grep for method calls, check imports
2. **Identify the core** - what's actually being used?
3. **Strip ruthlessly** - delete everything unused
4. **Eliminate duplication** - remove duplicate methods, consolidate similar logic
5. **Standardise APIs** - use consistent parameter types and naming
6. **Simplify remaining** - reduce complexity in what's left
7. **Test thoroughly** - ensure refactoring doesn't break tests

## Red Flags During Code Review

### Questions to Ask
- "Is this method actually called anywhere?"
- "Do we have tests that use these scenarios?"
- "Could this be simpler and still meet the need?"
- "Are we building for hypothetical future requirements?"
- "Are there duplicate methods doing the same thing?"
- "Is the API consistent across similar classes?"
- "Are we using proper test assertions instead of throwing errors?"

### Immediate Actions
- **Delete unused code** - don't leave it "just in case"
- **Simplify complex options** - reduce to what's actually needed
- **Question comprehensive coverage** - is it being used?
- **Check test patterns** - align infrastructure with actual usage

## Remember
**The best test infrastructure is the simplest one that meets your actual needs.**

Test code should be:
- **Easy to understand** - simple and focused
- **Easy to maintain** - minimal complexity
- **Actually used** - every line serves a purpose
- **Aligned with reality** - matches how tests actually work

**Key Principle**: "Build what you need, when you need it, not what you think you might need."
