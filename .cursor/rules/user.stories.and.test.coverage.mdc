---
description:
globs:
alwaysApply: false
---
# User Stories and Test Coverage

This rule provides guidance on how user stories are organised and linked to feature files, enabling effective test coverage assessment and requirements traceability.

## User Stories Structure

User stories are stored in `.cursor/user-stories/` following this structure:

### File Naming Convention
- **Pattern**: `ML-{number}.{descriptive.name}.mdc`
- **Examples**: 
  - `ML-1.provide.project.name.and.create.exemption.mdc`
  - `ML-9.view.the.task.list.mdc`
  - `ML-12.provide.or.withhold.public.register.content.mdc`

### Story Content Structure
Each user story file contains:
- **User Story**: AS/I WANT/SO THAT format
- **Background**: Context and purpose
- **Resources**: Links to prototypes, design system components
- **Out of Scope**: Clear boundaries
- **Questions and Answers**: Design decisions and clarifications
- **Screenshots**: Visual descriptions of expected UI
- **Acceptance Criteria**: GIVEN/WHEN/THEN format with specific validation rules

## Feature File Linkage

### @issue Tags
Feature files in `test/features/` use `@issue=ML-{number}` tags to link to user stories:

```gherkin
@issue=ML-1 @issue=ML-9
Feature: Project name entry and navigation
```

### Finding Linked Tests
To find feature files for a specific user story:
```bash
grep -r "@issue=ML-{number}" test/features/
```

### Coverage Assessment
Use this approach to assess coverage:

1. **Story → Tests**: Check each user story has corresponding `@issue=ML-{number}` tags in feature files
2. **Tests → Stories**: Verify all feature files reference valid user stories
3. **Acceptance Criteria**: Ensure each AC has corresponding test scenarios
4. **Gap Analysis**: Identify stories without tests or tests without stories

## Coverage Mapping Examples

| Story | Feature Files | Coverage Status |
|-------|---------------|-----------------|
| ML-1 | `project.name.feature` | ✅ Complete |
| ML-9 | `view.task.list.feature`, `project.name.feature` | ✅ Complete |
| ML-12 | `public.register.consent.feature` | ✅ Complete |

## Using This for Test Development

### When Creating Tests
1. **Reference the user story**: Start with `.cursor/user-stories/ML-{number}.*.mdc`
2. **Tag appropriately**: Add `@issue=ML-{number}` to feature files
3. **Cover all ACs**: Ensure each acceptance criteria has test scenarios
4. **Match screenshots**: Verify tests validate the UI elements described

### When Assessing Coverage
1. **Check the README**: `.cursor/user-stories/README.md` provides overview
2. **Verify linkage**: Ensure all stories have feature files with correct @issue tags
3. **Validate completeness**: Each AC should have corresponding scenarios
4. **Check test quality**: Tests should reflect realistic user journeys per the user stories

### When Adding New Stories
1. **Create story file**: Following naming convention in `.cursor/user-stories/`
2. **Update README**: Add entry to the overview table
3. **Create feature file**: In `test/features/` with appropriate @issue tag
4. **Implement tests**: Covering all acceptance criteria
5. **Update coverage status**: Mark as complete when tests are implemented

## Test Coverage Commands

### Find stories without tests
```bash
# Get all story numbers from filenames
ls .cursor/user-stories/ML-*.mdc | grep -o "ML-[0-9]*" | sort -u > stories.txt

# Get all @issue tags from features
grep -r "@issue=ML-" test/features/ | grep -o "ML-[0-9]*" | sort -u > tested.txt

# Find differences
comm -23 stories.txt tested.txt
```

### Find tests without stories
```bash
# Reverse the comparison
comm -13 stories.txt tested.txt
```

## Quality Guidelines

### User Story Quality
- **Clear acceptance criteria** with specific validation rules
- **Realistic scenarios** that reflect actual user behaviour
- **Domain-appropriate language** using marine licensing terminology
- **Screenshot descriptions** that match actual UI implementation

### Test Quality
- **Screenplay pattern**: Use tasks and interactions appropriately
- **Domain language**: Test steps should use marine licensing terms
- **Realistic data**: Test scenarios should use appropriate marine project examples
- **Error scenarios**: Cover validation and error cases from ACs

## Integration with Existing Rules

This rule works alongside:
- **application.under.test**: Provides domain context for realistic test scenarios
- **personas**: Informs test scenarios with realistic user behaviour patterns
- **screenplay-pattern**: Defines how tests should be structured and implemented
- **project-structure**: Defines where files should be located

## Maintenance

Keep the `.cursor/user-stories/README.md` updated as the source of truth for:
- Story status tracking
- Feature file mappings
- Coverage overview
- Navigation between requirements and tests
