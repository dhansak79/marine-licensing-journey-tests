---
description: This rule applies to ALL code generation, refactoring, and test automation work. I have a tendency to over-engineer solutions and create unnecessary complexity.
globs: 
alwaysApply: false
---
# Stop Over-Engineering and Making Stuff Up

*Based on repeated mistakes in marine licensing test automation refactoring*

## Context
This rule applies to ALL code generation, refactoring, and test automation work. I have a tendency to over-engineer solutions and create unnecessary complexity.

## ðŸŽ¯ BREAKTHROUGH: "Check Actual Usage First"

### The Phantom Class Discovery
**Always trace actual usage, not just imports!**

```bash
# Found ExemptionModel imported everywhere but NEVER used
grep -r "new ExemptionModel" .     # No results!
grep -r "ExemptionModel\." .       # No method calls!
grep -r "import.*ExemptionModel" . # Imported but unused!
```

**The Lesson:** Imports lie. Method calls tell the truth.

**The Process:**
1. **Trace actual instantiation** - `grep -r "new ClassName"`
2. **Find method calls** - `grep -r "ClassName\."`
3. **Check real usage** - Don't trust imports/exports
4. **Delete phantom code** - If it's not called, it's waste

This single check revealed an entire unused class architecture!

## Core Problems I Keep Making

### 1. Over-Engineering Test Data Models
**What I do wrong**:
- Create massive arrays of fake data that aren't used
- Build complex generation methods for "all possible scenarios"
- Add persona-based generators when tests don't use personas
- Create comprehensive validation methods that aren't called

**What I should do**:
- Check actual usage with `grep` BEFORE building anything
- Start with the absolute minimum needed
- Only add complexity when there's a clear, immediate need

### 2. Making Up Requirements
**What I do wrong**:
- Assume tests need "realistic" data when simple strings work fine
- Create boundary testing utilities before checking if boundary tests exist
- Build comprehensive error handling when simple assertions suffice
- Add features "just in case" they might be useful

**What I should do**:
- Look at actual test usage patterns
- Ask "Is this actually needed RIGHT NOW?"
- Build only what's being used

### 3. Creating Unnecessary Abstractions
**What I do wrong**:
- Build abstract base classes for simple utilities
- Create plugin architectures for straightforward helpers
- Add configuration systems for hardcoded values
- Design "extensible" systems when simple functions work

**What I should do**:
- Use simple functions and classes
- Avoid abstractions until there's clear duplication
- Prefer composition over inheritance

### 4. Ignoring YAGNI (You Aren't Gonna Need It)
**What I do wrong**:
- Build for hypothetical future requirements
- Create comprehensive test scenarios that aren't used
- Add "flexibility" that complicates simple use cases
- Design for scale when dealing with small test suites

**What I should do**:
- Build for TODAY'S requirements
- Add features only when they're actually needed
- Keep it simple and focused

## Mandatory Checks Before Building Anything

### 1. Usage Audit
```bash
# Always run these commands FIRST
grep -r "ClassName" .
grep -r "methodName" .
```

### 2. Ask These Questions
- "Is this method actually called anywhere?"
- "Do existing tests use this complexity?"
- "Could this be simpler and still work?"
- "Am I building for hypothetical requirements?"
- "What's the MINIMUM that would work?"

### 3. Start Minimal
- Begin with the simplest possible implementation
- Add complexity only when tests fail without it
- Prefer explicit over clever
- Choose boring solutions over interesting ones

## Red Flags That I'm Over-Engineering

### In Code
- More than 50 lines for a simple data model
- Arrays with more than 5-10 items when 2-3 would work
- Methods that aren't called anywhere
- Complex inheritance hierarchies for simple data
- Configuration files for hardcoded values

### In Thinking
- "This might be useful later"
- "Let's make it extensible"
- "We should handle all edge cases"
- "This needs to be realistic"
- "What if someone wants to..."

## The Refactoring Pattern That Works

1. **Audit actual usage** - grep for method calls, check imports
2. **Identify the core** - what's actually being used?
3. **Strip ruthlessly** - delete everything unused
4. **Eliminate duplication** - remove duplicate methods
5. **Standardise APIs** - use consistent parameter types
6. **Simplify remaining** - reduce complexity in what's left
7. **Test thoroughly** - ensure refactoring doesn't break tests

## Examples of Good Simplification

### Before (Over-engineered)
```javascript
class MarineProjectModel {
  static MARINE_ACTIVITIES = [/* 20+ items */]
  static MARINE_LOCATIONS = [/* 15+ items */]
  static PROJECT_TYPES = [/* 10+ items */]
  
  static generateForPersona(persona) { /* complex logic */ }
  static generateBoundaryTestData() { /* unused method */ }
  static generateCompleteProject() { /* over-complex */ }
}
```

### After (Simple and Focused)
```javascript
class MarineProjectModel {
  static generateProjectName() {
    const activities = ['Wind Farm', 'Cable Installation', 'Marina Construction']
    const locations = ['Norfolk Coast', 'Thames Estuary', 'Bristol Channel']
    return `${faker.helpers.arrayElement(locations)} ${faker.helpers.arrayElement(activities)}`
  }
  
  static generateOversizedProjectName() {
    return 'x'.repeat(251) // Simple boundary test
  }
}
```

## Remember
- **Simple beats clever**
- **Working beats perfect**
- **Used beats comprehensive**
- **Boring beats interesting**

> "As little as possible, as much as necessary" - Shane Kelly

The goal is working tests, not impressive code architecture.
